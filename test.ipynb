{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "os.system(\"export PYTHONPATH=`readlink -f ./`\")\n",
    "from config import *\n",
    "from houses import TEST_HOUSE,TRAIN_HOUSE\n",
    "from archive.load_tepco import load_house_dataset_by_houses,load_house_dataset_by_houses_ex\n",
    "from time2graph.utils.base_utils import Debugger\n",
    "from time2graph.core.model_TEPCO import Time2Graph\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from time2graph.core.shapelet_embedding import ShapeletEmbedding\n",
    "from time2graph.core.shapelet_utils import transition_matrixs,__mat2edgelist,graph_embedding\n",
    "testhouse = [str(i).zfill(3) for i in TEST_HOUSE]\n",
    "trainhouse = [str(i).zfill(3) for i in TRAIN_HOUSE]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參數集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav='sleep'\n",
    "class args:\n",
    "    pass\n",
    "args.seg_length,args.num_segment =5,5\n",
    "args.cutpoints=[(0,3),(2,5)]\n",
    "args.behav=behav\n",
    "args.dataset = args.behav\n",
    "args.K,args.C=20,40\n",
    "args.opt_metric = 'accuracy'\n",
    "args.init,args.warp=0,2\n",
    "args.gpu_enable =True\n",
    "args.tflag=True\n",
    "args.embed,args.cmethod ='aggregate','greedy'\n",
    "args.percentile =10\n",
    "args.batch_size,args.embed_size =16,16\n",
    "args.njobs=5\n",
    "args.optimizer ,args.measurement='Adam','gdtw'\n",
    "args.alpha,args.beta=0.1,0.05\n",
    "args.scaled,args.norm=True,False\n",
    "args.no_global = True\n",
    "args.multi_graph,args.data_size =False,1\n",
    "args.kernel,args.feature = 'dts','all'\n",
    "args.n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.kernel=='xgb':\n",
    "    opt_args= {\n",
    "                'max_depth': 16,\n",
    "                'learning_rate': 0.2,\n",
    "                'scale_pos_weight': 1,\n",
    "                'booster': 'gbtree'\n",
    "            }\n",
    "elif (args.kernel=='dts') or (args.kernel=='rf'):\n",
    "    opt_args={\n",
    "                'criterion': 'gini',\n",
    "                'max_features': 'auto',\n",
    "                'max_depth': 10,\n",
    "                'min_samples_split': 4,\n",
    "                'min_samples_leaf': 3,\n",
    "                'class_weight': 'balanced'\n",
    "            }\n",
    "else:\n",
    "    raise ValueError(\"please choose a classifier\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info]data shape 7296x25\n",
      "[info]training: 0.34 positive ratio with 7296\n",
      "[info]test: 0.37 positive ratio with 1824\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test,z_train,z_test = load_house_dataset_by_houses_ex(\n",
    "        TEST_HOUSE=testhouse,TRAIN_HOUSE=trainhouse,assign_behavior=behav)\n",
    "Debugger.info_print('data shape {}x{}'.format(x_train.shape[0],x_train.shape[1]))\n",
    "Debugger.info_print('training: {:.2f} positive ratio with {}'.format(float(sum(y_train) / len(y_train)),\n",
    "                                                                        len(y_train)))\n",
    "Debugger.info_print('test: {:.2f} positive ratio with {}'.format(float(sum(y_test) / len(y_test)),\n",
    "                                                                    len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info]Time2GraphEmbed\n",
      "[info]initialize t2g model with {'kernel': 'dts', 'kwargs': {'candidate_method': 'greedy', 'njobs': 5, 'optimizer': 'Adam', 'representation_size': 16, 'feature_mode': 'all', 'label_all': 'sleep', 'cutpoints': [(0, 3), (2, 5)]}, 'K': 20, 'C': 40, 'seg_length': 5, 'warp': 2, 'tflag': True, 'opt_metric': 'accuracy', 'mode': 'aggregate', 'batch_size': 16, 'gpu_enable': True, 'percentile': 10, 'shapelets': None, 'sembeds': None, 'clf': None, 'lr': 0.01, 'p': 2, 'alpha': 0.1, 'beta': 0.05, 'multi_graph': False, 'debug': True, 'measurement': 'gdtw', 'verbose': False, 'global_flag': True, 'cutpoints': [(0, 3), (2, 5)]}\n"
     ]
    }
   ],
   "source": [
    "m = Time2Graph(kernel=args.kernel, K=args.K, C=args.C, seg_length=args.seg_length,\n",
    "                opt_metric=args.opt_metric, init=args.init, gpu_enable=args.gpu_enable,\n",
    "                warp=args.warp, tflag=args.tflag, mode=args.embed,\n",
    "                percentile=args.percentile, candidate_method=args.cmethod,\n",
    "                batch_size=args.batch_size, njobs=args.njobs,\n",
    "                optimizer=args.optimizer, alpha=args.alpha,\n",
    "                beta=args.beta, measurement=args.measurement,\n",
    "                representation_size=args.embed_size, data_size=args.data_size,\n",
    "                scaled=args.scaled, norm=args.norm, global_flag=args.no_global,\n",
    "                multi_graph=args.multi_graph,\n",
    "                shapelets_cache='{}/scripts/cache/{}_{}_{}_{}_shapelets.cache'.format(\n",
    "                    module_path, \n",
    "                    args.dataset, \n",
    "                    args.cmethod, args.K, args.seg_length),\n",
    "                    feature_mode = args.feature,\n",
    "                    label_all = args.behav,\n",
    "                    cutpoints=args.cutpoints,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info]learn_shapelets\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '{}/scripts/cache/{}/'.format(module_path, args.dataset)\n",
    "m.learn__shapelet(X=x_train, Y=y_train,Z=z_train, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info]scaled embedding model...\n"
     ]
    }
   ],
   "source": [
    "assert m.t2g.sembeds is None\n",
    "for k in range(m.data_size):\n",
    "    m.data_scaler[k].fit(x_train[:, :, k])\n",
    "X_scaled = np.zeros(x_train.shape, dtype=np.float)\n",
    "for k in range(m.data_size):\n",
    "    X_scaled[:, :, k] = m.data_scaler[k].fit_transform(x_train[:, :, k])\n",
    "X_scaled = np.zeros(x_train.shape, dtype=np.float)\n",
    "if args.scaled:\n",
    "    Debugger.info_print('scaled embedding model...')\n",
    "    inputx=X_scaled\n",
    "else:\n",
    "    Debugger.info_print('unscaled embedding model...')\n",
    "    inputx=x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert m.t2g.shapelets is not None, 'shapelets has not been learnt yet'\n",
    "m.t2g.sembeds = ShapeletEmbedding(\n",
    "    seg_length=args.seg_length, tflag=args.tflag, multi_graph=args.multi_graph,\n",
    "    cache_dir=cache_dir, tanh=False, debug=m.t2g.debug,\n",
    "    percentile=args.percentile, measurement=args.measurement, mode=args.embed,\n",
    "    global_flag=args.no_global, \n",
    "    **m.t2g.kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info]weight matrix between shapelet 0 15\n",
      "[info](4803, 15, 1)\n",
      "[info]threshold(10) 0.18452430814504625, mean 0.9767476916313171\n",
      "[info]4803x3\n",
      "[info]27030 edges involved in shapelets graph\n",
      "[info]weight matrix between shapelet 10 25\n",
      "[info](4803, 15, 1)\n",
      "[info]threshold(10) 0.18448342978954319, mean 0.9765496253967285\n",
      "[info]4803x3\n",
      "[info]27199 edges involved in shapelets graph\n"
     ]
    }
   ],
   "source": [
    "# m.t2g.sembeds.fit(time_series_set=x_train[np.argwhere(y_train == 0).reshape(-1), :, :],\n",
    "#                     shapelets=m.t2g.shapelets, warp=args.warp, init=args.init)\n",
    "transition_set=transition_matrixs(\n",
    "            time_series_set=x_train[np.argwhere(y_train == 0).reshape(-1), :, :], \n",
    "            shapelets=m.t2g.shapelets, seg_length=args.seg_length,\n",
    "            tflag=args.tflag, multi_graph=args.multi_graph, tanh=False, debug=True,\n",
    "            init=args.init, warp=args.warp, percentile=args.percentile, threshold=-1,\n",
    "            measurement=args.measurement, global_flag=args.no_global,\n",
    "            cutpoints=args.cutpoints\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,transition in enumerate(transition_set):\n",
    "    tmat, sdist, dist_threshold = transition\n",
    "    m.t2g.sembeds.dist_threshold = dist_threshold\n",
    "    m.t2g.sembeds.embeddings.append(\n",
    "        graph_embedding(\n",
    "        tmat=tmat, num_shapelet=len(shapelets), embed_size=args.embed_size,\n",
    "        cache_dir=args.cache_dir, **self.deepwalk_args)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info]embedding threshold 0.18448342978954319\n",
      "[info]sdist size (7296, 3, 20)\n",
      "[info]sdist size (7296, 3, 20)\n",
      "[info]using setup parameters executed by 100.00%\n",
      "[info]classifier fit\n"
     ]
    }
   ],
   "source": [
    "x = m.extract_features(X=x_train,Z=z_train, init=args.init,mode=args.feature)\n",
    "max_accu, max_prec, max_recall, max_f1, max_metric = -1, -1, -1, -1, -1\n",
    "metric_measure = m.return_metric_method(opt_metric=m.t2g.opt_metric)\n",
    "m.train_classfit(x=x,Y=y_train,Z=z_train,n_splits=5,opt_args=opt_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info]embedding threshold 0.18448342978954319\n",
      "[info]sdist size (1824, 3, 20)\n",
      "[info]sdist size (1824, 3, 20)\n",
      "[debug]time series embedding executed by 100.00%\r"
     ]
    }
   ],
   "source": [
    "y_pred = m.predict(X=x_test,Z=z_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訊息已成功發送到Discord Webhook！\n"
     ]
    }
   ],
   "source": [
    "Debugger.dc_print('{}\\n{:.2f} positive ratio\\nresult: accu {:.4f}, prec {:.4f}, recall {:.4f}, f1 {:.4f}'.format(\n",
    "        args.cutpoints,float(sum(y_test) / len(y_test)),                                                                           \n",
    "            accuracy_score(y_true=y_test, y_pred=y_pred),\n",
    "            precision_score(y_true=y_test, y_pred=y_pred),\n",
    "            recall_score(y_true=y_test, y_pred=y_pred),\n",
    "            f1_score(y_true=y_test, y_pred=y_pred)\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2G",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
